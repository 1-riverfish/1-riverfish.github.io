<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="1-riverfish"><title>两只小爬虫 · Ginger's blog</title><meta name="description" content="两只小爬虫

冯兄推荐文学院的洪学姐，帮忙爬取旅游网站的数据用在毕业论文里，其一是出行矩阵，其二是词频统计，目标网站是马蜂窝和携程，反爬虫做的很烂，可以用Request库,还好两天加班加点把矩阵和词频统计都做出来了，学姐之前还开玩笑说做不完就毕不了业，现在应该可以毕业了:smile:
代码写了两部分"><meta name="keywords" content=""><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/" class="a-title">Share&amp;Joy</a></h3><h1 tabindex="-1" class="site-title-large"><a href="/" class="a-title">Ginger' Blog</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li><li class="soc"><a href="https://github.com/1-riverfish" target="_blank" rel="noopener noreferrer"><i class="fa fa-github">&nbsp;</i></a><a href="http://yoursite.com/atom.xml" target="_blank" rel="noopener noreferrer"><i class="fa fa-rss">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2018&nbsp;<a target="_blank" href="http://yoursite.com" rel="noopener noreferrer">1-riverfish</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>两只小爬虫</a></p><p class="post-meta"><span class="date meta-item">发布于&nbsp;2018-05-13</span><span class="meta-item"><i class="fa fa-tag"></i><span>&nbsp;</span><a href="/tags/Python/" title="Python" class="a-tag">Python</a><span>&nbsp;</span><a href="/tags/爬虫/" title="爬虫" class="a-tag">爬虫</a><span>&nbsp;</span><a href="/tags/Request库/" title="Request库" class="a-tag">Request库</a><span>&nbsp;</span></span></p><p class="post-abstract"><h1 id="两只小爬虫"><a href="#两只小爬虫" class="headerlink" title="两只小爬虫"></a>两只小爬虫</h1><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=25867002&auto=1&height=32"></iframe>

<p>冯兄推荐文学院的洪学姐，帮忙爬取旅游网站的数据用在毕业论文里，其一是出行矩阵，其二是词频统计，目标网站是马蜂窝和携程，<strong>反爬虫做的很烂</strong>，可以用Request库,还好两天加班加点把矩阵和词频统计都做出来了，学姐之前还开玩笑说做不完就毕不了业，现在应该可以毕业了:smile:</p>
<p>代码写了两部分，第一个代码是爬取游记链接并保存到文件，第二段代码是爬取游记文本并进行文本分析</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取携程的第一个代码 </span></span><br><span class="line"><span class="comment"># Stage_1</span></span><br><span class="line"><span class="comment"># @author:1-riverfish</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> os, io, sys, re, time, base64, json</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line">socket.setdefaulttimeout(<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHtml</span><span class="params">(url)</span>:</span></span><br><span class="line">    my_headers = [<span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>]</span><br><span class="line">    req = Request(url)</span><br><span class="line">    req.add_header(<span class="string">"User-Agent"</span>,my_headers[<span class="number">0</span>])</span><br><span class="line">    req.add_header(<span class="string">"GET"</span>,url)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#随机等待  反爬虫</span></span><br><span class="line">    time.sleep(random.randint(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    html = urlopen(req)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line">url_header = <span class="string">"http://you.ctrip.com/searchsite/travels/?query=%E5%8D%83%E5%B2%9B%E6%B9%96&amp;isAnswered=&amp;isRecommended=&amp;publishDate=&amp;PageNo="</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#一共有  293页</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">file=open(<span class="string">'xc_data.txt'</span>,<span class="string">'w'</span>) </span><br><span class="line">hrefList = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">294</span>):</span><br><span class="line">    <span class="comment"># url 没有问题</span></span><br><span class="line">    <span class="comment"># 解析  游记链接   </span></span><br><span class="line">    <span class="comment"># 解析日期  时间周期   2013-2017</span></span><br><span class="line">    url = url_header + str(i)</span><br><span class="line">    print(url)</span><br><span class="line">    </span><br><span class="line">    html = getHtml(url)</span><br><span class="line">    <span class="comment">#html.read().decode("utf-8")</span></span><br><span class="line">    bsObj = BeautifulSoup(html,<span class="string">"lxml"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#使用正则表达式匹配</span></span><br><span class="line">    aList = bsObj.findAll(<span class="string">"a"</span>,&#123;<span class="string">"href"</span>:re.compile(<span class="string">"/travels/[a-z]+"</span>)&#125;)</span><br><span class="line">    <span class="comment">#print(aList)</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> aList:</span><br><span class="line">        href = a.get(<span class="string">"href"</span>)</span><br><span class="line">        <span class="comment">#print(href)</span></span><br><span class="line">        hrefList.append(href)</span><br><span class="line">    <span class="comment">#print(hrefList)</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#写入文件</span></span><br><span class="line">file.write(str(hrefList))</span><br><span class="line"><span class="comment"># 关闭文件</span></span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取携程的第二段代码</span></span><br><span class="line"><span class="comment"># Stage_1</span></span><br><span class="line"><span class="comment"># @author:1-riverfish</span></span><br><span class="line"><span class="comment">#  从文本中读取数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'xc_data.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="comment">#txt中所有字符串读入data  </span></span><br><span class="line">    data = f.readlines()  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 将 data.txt文件的头尾中括号去掉</span></span><br><span class="line">    <span class="comment">#将单个数据分隔开存好 得到urlList  </span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> data:</span><br><span class="line">        urlList = line.split(<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去掉重复项</span></span><br><span class="line">urlList = urlList[::<span class="number">2</span>]</span><br><span class="line">linkList =[]</span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urlList:</span><br><span class="line">    <span class="comment">#  得到干净的url</span></span><br><span class="line">    url = url.strip(<span class="string">" "</span>)</span><br><span class="line">    url = url.strip(<span class="string">'\''</span>)</span><br><span class="line">    url = url.strip(<span class="string">"http://you.ctrip.com"</span>)</span><br><span class="line">    url = <span class="string">"http://you.ctrip.com/tr"</span>+ url</span><br><span class="line">    <span class="comment">#print(url)</span></span><br><span class="line">    linkList.append(url)</span><br><span class="line">print(len(linkList))</span><br><span class="line"></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> os, io, sys, re, time, base64, json</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line">socket.setdefaulttimeout(<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHtml</span><span class="params">(url)</span>:</span></span><br><span class="line">    my_headers = [<span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>]</span><br><span class="line">    req = Request(url)</span><br><span class="line">    req.add_header(<span class="string">"User-Agent"</span>,my_headers[<span class="number">0</span>])</span><br><span class="line">    req.add_header(<span class="string">"GET"</span>,url)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#随机等待  反爬虫</span></span><br><span class="line">    time.sleep(random.randint(<span class="number">1</span>,<span class="number">2</span>)/<span class="number">10</span>)</span><br><span class="line">    html = urlopen(req)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"><span class="comment">#  生成  全为  0 的矩阵</span></span><br><span class="line">matrix = np.zeros([<span class="number">29</span>,<span class="number">29</span>])</span><br><span class="line"><span class="comment"># 携程一次爬取是否可行?感觉不行</span></span><br><span class="line"><span class="comment">#  爬取下来的游记 先不保存  因为太多了</span></span><br><span class="line"><span class="comment">#count_1  记录次数</span></span><br><span class="line">count_1 = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> linkList:</span><br><span class="line">    print(url)</span><br><span class="line">    html = getHtml(url)</span><br><span class="line">    print(html.getcode(),count_1)</span><br><span class="line">    bsObj = BeautifulSoup(html,<span class="string">"lxml"</span>)</span><br><span class="line">    count_1 += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  获取游记文本内容</span></span><br><span class="line">    content = <span class="string">""</span></span><br><span class="line">    <span class="comment">#  这边获取文本内容不是很准确</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> bsObj.findAll(<span class="string">"p"</span>):</span><br><span class="line">        part = p.get_text()</span><br><span class="line">        content += part</span><br><span class="line">    <span class="comment">#print(content)</span></span><br><span class="line">    <span class="comment">#  得到大概完整的文本内容  进行顺序解析</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">##  这里要考虑一下，景点名称的选择很重要</span></span><br><span class="line">    visitList = [<span class="string">"中心湖区"</span>,<span class="string">"东南湖区"</span>,<span class="string">"秀水广场"</span>,<span class="string">"千岛湖广场"</span>,<span class="string">"水之灵"</span>,<span class="string">"明珠观光"</span>,<span class="string">"天屿"</span>,<span class="string">"梦姑塘"</span>,<span class="string">"森林氧吧"</span>,<span class="string">"牧心谷"</span>,<span class="string">"林海归真"</span>,<span class="string">"热气球"</span>,<span class="string">"钓鱼岛"</span>,<span class="string">"石林"</span>,<span class="string">"芹川"</span>,<span class="string">"龙川湾"</span>,<span class="string">"狮城"</span>,<span class="string">"九咆界"</span>,<span class="string">"啤酒风情小镇"</span>,<span class="string">" 大峡谷"</span>,<span class="string">"下姜"</span>,<span class="string">"汾口"</span>,<span class="string">"九龙溪漂流"</span>,<span class="string">"白云溪漂流"</span>,<span class="string">"王子谷漂流"</span>,<span class="string">"龙门峡谷漂流"</span>,<span class="string">"九潭峡谷漂流"</span>,<span class="string">"环湖骑行"</span>,<span class="string">"环岛骑行"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  这边有可能不能找到所有的数字</span></span><br><span class="line">    <span class="comment"># 先把数字全部找出来替换为空格</span></span><br><span class="line">    content = re.sub(<span class="string">"[0-9]"</span>,str(<span class="string">""</span>),content,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#print(content)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 再用数字替换景点</span></span><br><span class="line">    <span class="comment">#  进行景点顺序解析</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> visit <span class="keyword">in</span> visitList:</span><br><span class="line">        content = re.sub(visit,str(count),content,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">        count+=<span class="number">1</span></span><br><span class="line">    <span class="comment">#  检验一下是否将景点替换为数字</span></span><br><span class="line">    <span class="comment"># print(content)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 得到数字列表即对应顺序  下边这种正则写的方式有问题</span></span><br><span class="line">    <span class="comment">#  提取元文本中的数字</span></span><br><span class="line">    sequenceList = re.findall(<span class="string">"\d+"</span>,content,flags=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#  检验提取数字的顺序  没有问题 </span></span><br><span class="line">    print(sequenceList)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  根据相邻元素填写矩阵</span></span><br><span class="line">    size = len(sequenceList)</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">0</span>,size<span class="number">-1</span>):</span><br><span class="line">        num1 = int(sequenceList[num])</span><br><span class="line">        num2 = int(sequenceList[num+<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> num1 != num2:</span><br><span class="line">            matrix[num1%<span class="number">29</span>][num2%<span class="number">29</span>]+=<span class="number">1</span></span><br><span class="line">        <span class="comment">#  相等则不进行操作</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#  爬取完所有文章  输出矩阵</span></span><br><span class="line">print(matrix)</span><br><span class="line"></span><br><span class="line">np.savetxt(<span class="string">'matrix.csv'</span>, matrix, delimiter = <span class="string">','</span>)</span><br></pre></td></tr></table></figure>
<p>统计词频的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  从文本中读取数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'xc_data.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="comment">#txt中所有字符串读入data  </span></span><br><span class="line">    data = f.readlines()  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 将 data.txt文件的头尾中括号去掉</span></span><br><span class="line">    <span class="comment">#将单个数据分隔开存好 得到urlList  </span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> data:</span><br><span class="line">        urlList = line.split(<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去掉重复项</span></span><br><span class="line">urlList = urlList[::<span class="number">2</span>]</span><br><span class="line">linkList =[]</span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urlList:</span><br><span class="line">    <span class="comment">#  得到干净的url</span></span><br><span class="line">    url = url.strip(<span class="string">" "</span>)</span><br><span class="line">    url = url.strip(<span class="string">'\''</span>)</span><br><span class="line">    url = url.strip(<span class="string">"http://you.ctrip.com"</span>)</span><br><span class="line">    url = <span class="string">"http://you.ctrip.com/tr"</span>+ url</span><br><span class="line">    <span class="comment">#print(url)</span></span><br><span class="line">    linkList.append(url)</span><br><span class="line">print(len(linkList))</span><br><span class="line"></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> os, io, sys, re, time, base64, json</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line">socket.setdefaulttimeout(<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHtml</span><span class="params">(url)</span>:</span></span><br><span class="line">    my_headers = [<span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>]</span><br><span class="line">    req = Request(url)</span><br><span class="line">    req.add_header(<span class="string">"User-Agent"</span>,my_headers[<span class="number">0</span>])</span><br><span class="line">    req.add_header(<span class="string">"GET"</span>,url)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#随机等待  反爬虫</span></span><br><span class="line">    time.sleep(random.randint(<span class="number">1</span>,<span class="number">2</span>)/<span class="number">10</span>)</span><br><span class="line">    html = urlopen(req)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 统计每个景点出现的词频</span></span><br><span class="line">csvFile1 = open(<span class="string">'wordFre4.csv'</span>,<span class="string">'w'</span>, newline=<span class="string">''</span>,encoding=<span class="string">'utf-8-sig'</span>) </span><br><span class="line">writer2 = csv.writer(csvFile1)</span><br><span class="line"></span><br><span class="line">dic = &#123;<span class="string">"中心湖区"</span>:<span class="number">0</span>,<span class="string">"东南湖区"</span>:<span class="number">0</span>,<span class="string">"秀水广场"</span>:<span class="number">0</span>,<span class="string">"千岛湖广场"</span>:<span class="number">0</span>,<span class="string">"水之灵"</span>:<span class="number">0</span>,<span class="string">"明珠观光"</span>:<span class="number">0</span>,<span class="string">"天屿"</span>:<span class="number">0</span>,<span class="string">"梦姑塘"</span>:<span class="number">0</span>,<span class="string">"森林氧吧"</span>:<span class="number">0</span>,<span class="string">"牧心谷"</span>:<span class="number">0</span>,<span class="string">"林海归真"</span>:<span class="number">0</span>,<span class="string">"热气球"</span>:<span class="number">0</span>,<span class="string">"钓鱼岛"</span>:<span class="number">0</span>,<span class="string">"石林"</span>:<span class="number">0</span>,<span class="string">"芹川"</span>:<span class="number">0</span>,<span class="string">"龙川湾"</span>:<span class="number">0</span>,<span class="string">"狮城"</span>:<span class="number">0</span>,<span class="string">"九咆界"</span>:<span class="number">0</span>,<span class="string">"啤酒风情小镇"</span>:<span class="number">0</span>,<span class="string">"大峡谷"</span>:<span class="number">0</span>,<span class="string">"下姜"</span>:<span class="number">0</span>,<span class="string">"汾口"</span>:<span class="number">0</span>,<span class="string">"九龙溪漂流"</span>:<span class="number">0</span>,<span class="string">"白云溪漂流"</span>:<span class="number">0</span>,<span class="string">"王子谷漂流"</span>:<span class="number">0</span>,<span class="string">"龙门峡谷漂流"</span>:<span class="number">0</span>,<span class="string">"九潭峡谷漂流"</span>:<span class="number">0</span>,<span class="string">"环湖骑行"</span>:<span class="number">0</span>,<span class="string">"环岛骑行"</span>:<span class="number">0</span>&#125;</span><br><span class="line">count_1 = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> linkList[<span class="number">2432</span>:]:</span><br><span class="line">    print(url)</span><br><span class="line">    html = getHtml(url)</span><br><span class="line">    <span class="comment">#print(html.getcode(),count_1)</span></span><br><span class="line">    bsObj = BeautifulSoup(html,<span class="string">"lxml"</span>)</span><br><span class="line">    count_1 += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  获取游记文本内容</span></span><br><span class="line">    content = <span class="string">""</span></span><br><span class="line">    <span class="comment">#  这边获取文本内容不是很准确</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> bsObj.findAll(<span class="string">"p"</span>):</span><br><span class="line">        part = p.get_text()</span><br><span class="line">        content += part</span><br><span class="line">    <span class="comment">#  得到大概完整的文本内容  </span></span><br><span class="line"></span><br><span class="line">    <span class="comment">##  景点名称的选择很重要</span></span><br><span class="line">    visitList = [<span class="string">"中心湖区"</span>,<span class="string">"东南湖区"</span>,<span class="string">"秀水广场"</span>,<span class="string">"千岛湖广场"</span>,<span class="string">"水之灵"</span>,<span class="string">"明珠观光"</span>,<span class="string">"天屿"</span>,<span class="string">"梦姑塘"</span>,<span class="string">"森林氧吧"</span>,<span class="string">"牧心谷"</span>,<span class="string">"林海归真"</span>,<span class="string">"热气球"</span>,<span class="string">"钓鱼岛"</span>,<span class="string">"石林"</span>,<span class="string">"芹川"</span>,<span class="string">"龙川湾"</span>,<span class="string">"狮城"</span>,<span class="string">"九咆界"</span>,<span class="string">"啤酒风情小镇"</span>,<span class="string">"大峡谷"</span>,<span class="string">"下姜"</span>,<span class="string">"汾口"</span>,<span class="string">"九龙溪漂流"</span>,<span class="string">"白云溪漂流"</span>,<span class="string">"王子谷漂流"</span>,<span class="string">"龙门峡谷漂流"</span>,<span class="string">"九潭峡谷漂流"</span>,<span class="string">"环湖骑行"</span>,<span class="string">"环岛骑行"</span>]</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#在文本中查找  写成字典形式</span></span><br><span class="line">    <span class="keyword">for</span> visit <span class="keyword">in</span> visitList:</span><br><span class="line">        dic[visit] = dic[visit] + content.count(visit)</span><br><span class="line">        print(dic[visit])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> dic:</span><br><span class="line">    writer2.writerow([key, dic[key]])</span><br><span class="line">csvFile1.close()</span><br></pre></td></tr></table></figure>
<p>这周完成的第二件事情是和两位同学做数模比赛，很巧的是我们选的也是旅游类型的赛题，做路线规划，更巧的是又是我去爬数据(请携程爸爸放过我)，所以很多东西就很熟悉了，但是这份代码数据解析的部分真的烦，要做好多解析，话不多少，贴上代码还有爬取下来文件的百度云链接</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Stage1</span></span><br><span class="line"><span class="comment"># @author:1-riverfish</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[3]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> os, io, sys, re, time, base64, json</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line">socket.setdefaulttimeout(<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[4]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHtml</span><span class="params">(url)</span>:</span></span><br><span class="line">    my_headers = [<span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>]</span><br><span class="line">    req = Request(url)</span><br><span class="line">    req.add_header(<span class="string">"User-Agent"</span>,my_headers[<span class="number">0</span>])</span><br><span class="line">    req.add_header(<span class="string">"GET"</span>,url)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#随机等待  反爬虫</span></span><br><span class="line">    time.sleep(random.randint(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    html = urlopen(req)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[12]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">url_header = <span class="string">"http://www.mafengwo.cn/search/s.php?q=%E5%8D%97%E4%BA%AC&amp;p="</span></span><br><span class="line">url_tail = <span class="string">"&amp;t=poi&amp;kt=1"</span></span><br><span class="line"></span><br><span class="line">file=open(<span class="string">'module_data.txt'</span>,<span class="string">'w'</span>) </span><br><span class="line">hrefList = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">51</span>):</span><br><span class="line">    url = url_header + str(i) + url_tail</span><br><span class="line">    print(url)</span><br><span class="line">    </span><br><span class="line">    html = getHtml(url)</span><br><span class="line">    <span class="comment">#html.read().decode("utf-8")</span></span><br><span class="line">    bsObj = BeautifulSoup(html,<span class="string">"lxml"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#使用正则表达式匹配</span></span><br><span class="line">    aList = bsObj.findAll(<span class="string">"a"</span>,&#123;<span class="string">"href"</span>:re.compile(<span class="string">"http://www.mafengwo.cn/poi/[0-9]+"</span>)&#125;)</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> aList:</span><br><span class="line">        href = a.get(<span class="string">"href"</span>)</span><br><span class="line">        <span class="comment">#print(href)</span></span><br><span class="line">        hrefList.append(href)</span><br><span class="line"></span><br><span class="line">hrefList = hrefList[::<span class="number">4</span>]</span><br><span class="line"><span class="keyword">for</span> href <span class="keyword">in</span> hrefList:</span><br><span class="line">    print(href)</span><br><span class="line"></span><br><span class="line">print(len(hrefList))</span><br><span class="line"><span class="comment">#写入文件</span></span><br><span class="line">file.write(str(hrefList))</span><br><span class="line"><span class="comment"># 关闭文件</span></span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure>
<p>第二段代码进行解析，写的有点乱，没有进行优化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Stage2</span></span><br><span class="line"><span class="comment"># @author:1-riverfish</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[74]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#  从文本中读取数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'module_data.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="comment">#txt中所有字符串读入data</span></span><br><span class="line">    data = f.readlines()</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 将 data.txt文件的头尾中括号去掉</span></span><br><span class="line">    <span class="comment">#将单个数据分隔开存好 得到urlList  </span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> data:</span><br><span class="line">        urlList = line.split(<span class="string">','</span>)</span><br><span class="line">        </span><br><span class="line">linkList = []</span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urlList:</span><br><span class="line">    <span class="comment">#  得到干净的url</span></span><br><span class="line">    url = url.strip(<span class="string">" "</span>)</span><br><span class="line">    url = url.strip(<span class="string">'\''</span>)</span><br><span class="line">    <span class="comment">#print(url)</span></span><br><span class="line">    linkList.append(url)</span><br><span class="line"><span class="comment">#print(len(linkList))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[75]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> os, io, sys, re, time, base64, json</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line">socket.setdefaulttimeout(<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[76]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHtml</span><span class="params">(url)</span>:</span></span><br><span class="line">    my_headers = [<span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>]</span><br><span class="line">    req = Request(url)</span><br><span class="line">    req.add_header(<span class="string">"User-Agent"</span>,my_headers[<span class="number">0</span>])</span><br><span class="line">    req.add_header(<span class="string">"GET"</span>,url)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#随机等待  反爬虫</span></span><br><span class="line">    time.sleep(random.randint(<span class="number">1</span>,<span class="number">2</span>)/<span class="number">10.0</span>)</span><br><span class="line">    html = urlopen(req)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[77]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#  750条数据  分成七次</span></span><br><span class="line"></span><br><span class="line">count_1 =<span class="number">1</span></span><br><span class="line"><span class="comment"># 景点标号</span></span><br><span class="line">num = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开csv文件  注意设置 csv文件编码格式</span></span><br><span class="line">csvFile = open(<span class="string">'TF_750.csv'</span>,<span class="string">'w'</span>, newline=<span class="string">''</span>,encoding=<span class="string">'utf-8-sig'</span>) <span class="comment"># 设置newline，否则两行之间会空一行</span></span><br><span class="line">writer = csv.writer(csvFile)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> linkList[:]:</span><br><span class="line">    <span class="comment">#  存储相关信息的列表</span></span><br><span class="line">    tour_list = []</span><br><span class="line">    poi_id_list = re.findall(<span class="string">"\d+"</span>,url,flags=<span class="number">0</span>)</span><br><span class="line">    poi_id = poi_id_list[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#print(poi_id)</span></span><br><span class="line">    html = getHtml(url)</span><br><span class="line">    print(html.getcode(),count_1)</span><br><span class="line">    <span class="comment">#print(html.read().decode("utf-8"))</span></span><br><span class="line">    count_1 += <span class="number">1</span></span><br><span class="line">    <span class="comment">### num title price numcomment text time_refer1 time_refer2 traffic opentime</span></span><br><span class="line">    bsObj = BeautifulSoup(html,<span class="string">"lxml"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  解析得到数据  把每一行数据集中在一起</span></span><br><span class="line">    <span class="comment">#  景点名称</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        title = bsObj.find(<span class="string">"h1"</span>).get_text()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        title = <span class="string">"null"</span></span><br><span class="line">    print(num,title)</span><br><span class="line">    tour_list.append(num)</span><br><span class="line">    tour_list.append(title)</span><br><span class="line">    num += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#室内室外  根据title判断</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#门票价格</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        price_dt = bsObj.find(<span class="string">"dt"</span>,text=<span class="string">"门票"</span>)</span><br><span class="line">        price_dd = price_dt.next_sibling.next_sibling</span><br><span class="line">        price = (price_dd.div).get_text()</span><br><span class="line">        price_list = re.findall(<span class="string">"\d+元"</span>,price,flags=<span class="number">0</span>)</span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> price_list:</span><br><span class="line">            p = p.strip(<span class="string">""</span>)</span><br><span class="line">            p = float(p.strip(<span class="string">"元"</span>))</span><br><span class="line">            total += p</span><br><span class="line">        <span class="keyword">if</span> len(price_list) != <span class="number">0</span>:</span><br><span class="line">            price = total / len(price_list) </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            price = <span class="number">0</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        price = <span class="number">0</span></span><br><span class="line">    print(<span class="string">"门票价格"</span>,str(int(price)))</span><br><span class="line">    tour_list.append(int(price))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#点评数量</span></span><br><span class="line">    <span class="comment">#需要写成一个向量的形式</span></span><br><span class="line">    num_comment_a = bsObj.find(<span class="string">"a"</span>,&#123;<span class="string">"title"</span>:<span class="string">"蜂蜂点评"</span>&#125;)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        num_comment = (num_comment_a.span).get_text()</span><br><span class="line">        num_comment = num_comment.lstrip(<span class="string">"（"</span>)</span><br><span class="line">        num_comment = num_comment.rstrip(<span class="string">"）条"</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        num_comment = <span class="number">0</span></span><br><span class="line">    print(<span class="string">"点评数量"</span>,num_comment)</span><br><span class="line">    tour_list.append(num_comment)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  动态生成的评论</span></span><br><span class="line">    <span class="comment">#  请求  解析返回的json</span></span><br><span class="line">    json_url_head = <span class="string">'http://pagelet.mafengwo.cn/poi/pagelet/poiCommentListApi?&amp;params=&#123;"poi_id":"'</span></span><br><span class="line">    json_url_tail = <span class="string">'"&#125;'</span></span><br><span class="line">    json_url = json_url_head + poi_id + json_url_tail</span><br><span class="line">    <span class="comment"># print(json_url)</span></span><br><span class="line">    <span class="comment">#  得到 json  链接 进行请求</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        json_content = urlopen(json_url).read().decode(<span class="string">"utf-8"</span>)</span><br><span class="line">        json_content = json.loads(json_content)</span><br><span class="line">        <span class="comment">#  成功请求到 json  字符串  格式转换 得到想要的内容</span></span><br><span class="line">        <span class="comment">#  格式转换成功</span></span><br><span class="line">        review = json_content[<span class="string">"data"</span>][<span class="string">'html'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        review = <span class="string">""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  顺序固定  有图  好评  中评  差评  </span></span><br><span class="line">    text = []</span><br><span class="line">    text_num = []</span><br><span class="line">    text = re.findall(<span class="string">'\d+条'</span>,review,flags=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> titem <span class="keyword">in</span> text:</span><br><span class="line">        titem = titem.strip(<span class="string">""</span>)</span><br><span class="line">        titem = titem.strip(<span class="string">"条"</span>)</span><br><span class="line">        text_num.append(int(titem))</span><br><span class="line">    tour_list.append(text_num)</span><br><span class="line">    print(<span class="string">"评论列表 有图,好,中,差"</span>,text_num)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#用时参考</span></span><br><span class="line">    <span class="comment">#  默认参观时间优先级为  3</span></span><br><span class="line">    time_priority = <span class="number">3</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        time_list = bsObj.find(<span class="string">"li"</span>,&#123;<span class="string">"class"</span>:<span class="string">"item-time"</span>&#125;).get_text()</span><br><span class="line">        time_refer1 = re.findall(<span class="string">"\d+小时"</span>,time_list,flags=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        time_refer1 = []</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        time_refer2 = re.findall(<span class="string">"\d+天"</span>,time_list,flags=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        time_refer2 = []</span><br><span class="line">    <span class="comment">#  如果参观时间  包含天数 默认时间很长 分到第四等级</span></span><br><span class="line">    <span class="keyword">if</span> len(time_refer2) != <span class="number">0</span>:</span><br><span class="line">        time_priority = <span class="number">4</span></span><br><span class="line">    <span class="keyword">if</span> len(time_refer1) != <span class="number">0</span>:</span><br><span class="line">        time_ref = int((re.findall(<span class="string">"\d+"</span>,time_refer1[<span class="number">0</span>],flags=<span class="number">0</span>))[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> time_ref &lt;= <span class="number">2</span>:</span><br><span class="line">            time_priority = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> time_ref &lt;= <span class="number">4</span>:</span><br><span class="line">                time_priority = <span class="number">2</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> time_ref &lt;= <span class="number">6</span>:</span><br><span class="line">                    time_priority = <span class="number">3</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    time_priority = <span class="number">4</span></span><br><span class="line">    print(<span class="string">"参观时间优先级"</span>,time_priority)</span><br><span class="line">    tour_list.append(time_priority)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#交通   包含地铁及公交车</span></span><br><span class="line">    traffic_p = bsObj.find(<span class="string">"dt"</span>,text=<span class="string">"交通"</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        traffic = traffic_p.next_sibling.next_sibling</span><br><span class="line">        traffic = traffic.get_text()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        traffic = <span class="string">""</span></span><br><span class="line">    route_list = re.findall(<span class="string">'\d+路'</span>,traffic,flags=<span class="number">0</span>)</span><br><span class="line">    zhida = re.findall(<span class="string">"直达"</span>,traffic,flags=<span class="number">0</span>)</span><br><span class="line">    route_num = len(route_list)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#周围景点及交通</span></span><br><span class="line">    <span class="comment">#请求json   dis_list   周围地铁站的 J距离列表</span></span><br><span class="line">    jsonurl_head = <span class="string">'http://pagelet.mafengwo.cn/poi/pagelet/poiLocationApi?&amp;params=&#123;"poi_id":"'</span></span><br><span class="line">    jsonurl_tail = <span class="string">'"&#125;'</span></span><br><span class="line">    jsonurl = jsonurl_head + poi_id + jsonurl_tail</span><br><span class="line"></span><br><span class="line">    jsoncontent = urlopen(jsonurl).read().decode(<span class="string">"utf-8"</span>)</span><br><span class="line">    jsoncontent = json.loads(jsoncontent)</span><br><span class="line">    location = jsoncontent[<span class="string">"data"</span>][<span class="string">'html'</span>]</span><br><span class="line">    <span class="comment">#  先分割  再匹配</span></span><br><span class="line">    index = location.find(<span class="string">"位置-附近交通"</span>)</span><br><span class="line">    <span class="keyword">if</span> len(zhida) != <span class="number">0</span>:</span><br><span class="line">        dis_list = [<span class="number">0.0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dis_list = []</span><br><span class="line">    <span class="keyword">if</span> index != <span class="number">-1</span>:</span><br><span class="line">        location = location[index:]</span><br><span class="line">        dis_mlist = re.findall(<span class="string">"\d+[米]"</span>,location,flags=<span class="number">0</span>)</span><br><span class="line">        dis_glist = re.findall(<span class="string">"\d.\d+[公]"</span>,location,flags=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mitem <span class="keyword">in</span> dis_mlist:</span><br><span class="line">            mitem = mitem.strip(<span class="string">""</span>)</span><br><span class="line">            mitem = mitem.strip(<span class="string">"米"</span>)</span><br><span class="line">            mitem = str(float(mitem)/<span class="number">1000</span>)</span><br><span class="line">            dis_list.append(float(mitem))</span><br><span class="line">        <span class="keyword">for</span> gitem <span class="keyword">in</span> dis_glist:</span><br><span class="line">            gitem = gitem.strip(<span class="string">""</span>)</span><br><span class="line">            gitem = gitem.strip(<span class="string">"公"</span>)</span><br><span class="line">            dis_list.append(float(gitem))</span><br><span class="line">    <span class="comment">#距离列表</span></span><br><span class="line">    <span class="comment"># print(dis_list)两项放到一起</span></span><br><span class="line">    jiaotong = [dis_list,route_num]</span><br><span class="line">    print(<span class="string">"地铁距离向量 公交车数量"</span>,jiaotong)</span><br><span class="line">    tour_list.append(jiaotong)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#开放时间  格式的转换  XX:XX-YY:YY  选择第一个符合格式要求的时间写入</span></span><br><span class="line">    opentime_p = bsObj.find(<span class="string">"dt"</span>,text=<span class="string">"开放时间"</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        opentime = opentime_p.next_sibling.next_sibling</span><br><span class="line">        opentime = opentime.get_text()</span><br><span class="line">        opentime = re.findall(<span class="string">"\d+:\d+"</span>,opentime,flags=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> len(opentime) &gt;= <span class="number">2</span>:</span><br><span class="line">            opentimef = opentime[<span class="number">0</span>]+<span class="string">"-"</span>+opentime[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        opentimef = <span class="string">"00:00-24:00"</span></span><br><span class="line">    print(<span class="string">"开放时间"</span>,opentimef)</span><br><span class="line">    tour_list.append(opentimef)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#  写入一行到csv文件</span></span><br><span class="line">    writer.writerow(tour_list)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#关闭csv文件</span></span><br><span class="line">csvFile.close()</span><br></pre></td></tr></table></figure>
<p><a href="https://pan.baidu.com/s/1LF6VrIB4-f9lysFB3fH5Jg" target="_blank" rel="noopener">数据链接</a></p>
<p>密码: <strong>t3x7</strong></p>
<blockquote>
<p>今天好像还是母亲节……忘记给老妈打电话了……</p>
</blockquote>
</p></div><div class="share"><span>分享到</span>&nbsp;<span class="soc"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></span><span class="soc"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></span><span class="soc"><a href="http://twitter.com/home?status=http://yoursite.com/2018/05/13/两只小爬虫/%20Ginger's blog%20两只小爬虫" class="fa fa-twitter"></a></span></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/2018/05/15/搭建Chevereto图床/" title="搭建Chevereto图床"><i class="fa fa-angle-double-left"></i>&nbsp;上一篇: 搭建Chevereto图床</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/2018/05/08/Scrapy食用指南2/" title="Scrapy食用指南2">下一篇: Scrapy食用指南2&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2018&nbsp;<a target="_blank" href="http://yoursite.com" rel="noopener noreferrer">1-riverfish</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>