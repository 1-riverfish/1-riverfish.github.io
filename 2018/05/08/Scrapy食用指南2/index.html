<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="1-riverfish"><title>Scrapy食用指南2 · Ginger's blog</title><meta name="description" content="Scrapy文档笔记–2Scrapy指导假定你的Scrapy已经安装好，若没有安装好请看Scrapy文档笔记1
我们将要爬取quotes.toscrape.com,这篇文章将包含以下任务:

创建一个新的Scrapy项目
写一个Spider爬取站点并提取数据
使用命令行导出爬取的数据
改变爬虫使其递"><meta name="keywords" content=""><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/" class="a-title">Share&amp;Joy</a></h3><h1 tabindex="-1" class="site-title-large"><a href="/" class="a-title">Ginger' Blog</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li><li class="soc"><a href="https://github.com/1-riverfish" target="_blank" rel="noopener noreferrer"><i class="fa fa-github">&nbsp;</i></a><a href="http://yoursite.com/atom.xml" target="_blank" rel="noopener noreferrer"><i class="fa fa-rss">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2018&nbsp;<a target="_blank" href="http://yoursite.com" rel="noopener noreferrer">1-riverfish</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>Scrapy食用指南2</a></p><p class="post-meta"><span class="date meta-item">发布于&nbsp;2018-05-08</span><span class="meta-item"><i class="fa fa-tag"></i><span>&nbsp;</span><a href="/tags/Scrapy/" title="Scrapy" class="a-tag">Scrapy</a><span>&nbsp;</span></span></p><p class="post-abstract"><h1 id="Scrapy文档笔记–2"><a href="#Scrapy文档笔记–2" class="headerlink" title="Scrapy文档笔记–2"></a>Scrapy文档笔记–2</h1><h2 id="Scrapy指导"><a href="#Scrapy指导" class="headerlink" title="Scrapy指导"></a>Scrapy指导</h2><p>假定你的Scrapy已经安装好，若没有安装好请看<strong>Scrapy文档笔记1</strong></p>
<p>我们将要爬取quotes.toscrape.com,这篇文章将包含以下任务:</p>
<ol>
<li>创建一个新的Scrapy项目</li>
<li>写一个Spider爬取站点并提取数据</li>
<li>使用命令行导出爬取的数据</li>
<li>改变爬虫使其递归的寻找链接</li>
<li>使用spider参数</li>
</ol>
<p>Scrapy是用Python写的，如果你对Python还不熟悉，并且学过其他语言，请先阅读<a href="https://docs.python.org/3/tutorial/" target="_blank" rel="noopener">官方文档</a>或<a href="http://www.diveintopython3.net/" target="_blank" rel="noopener">Dive into Python3</a>；如果这是你第一次接触编程，你可以参考一下<a href="https://learnpythonthehardway.org/book/" target="_blank" rel="noopener">Learn Python the hard way</a>,当然你也可以参考<a href="https://wiki.python.org/moin/BeginnersGuide/NonProgrammers" target="_blank" rel="noopener">the list of Python resources for non-programmer</a></p>
<h2 id="创建一个项目"><a href="#创建一个项目" class="headerlink" title="创建一个项目"></a>创建一个项目</h2><p>在开始scraping前，你需要先创建一个新的Scrapy项目.进入你想要创建项目的目录，运行下面的命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject tutorial</span><br></pre></td></tr></table></figure>
<p>这会创建一个<code>tutorial</code>目录，包含下列内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tutorial/</span><br><span class="line">    scrapy.cfg            # deploy configuration file</span><br><span class="line"></span><br><span class="line">    tutorial/             # project&apos;s Python module, you&apos;ll import your code from here</span><br><span class="line">        __init__.py</span><br><span class="line"></span><br><span class="line">        items.py          # project items definition file</span><br><span class="line"></span><br><span class="line">        middlewares.py    # project middlewares file</span><br><span class="line"></span><br><span class="line">        pipelines.py      # project pipelines file</span><br><span class="line"></span><br><span class="line">        settings.py       # project settings file</span><br><span class="line"></span><br><span class="line">        spiders/          # a directory where you&apos;ll later put your spiders</span><br><span class="line">            __init__.py</span><br></pre></td></tr></table></figure>
<h2 id="我们的第一个爬虫"><a href="#我们的第一个爬虫" class="headerlink" title="我们的第一个爬虫"></a>我们的第一个爬虫</h2><p>Spiders是我们自己定义的，Scrapy用来从网页上爬取信息的的类.他必须是<code>scrapy.Spider</code>的子类并且定义了初始请求，在页面中寻找下一个链接，如何解析下载的界面并且提取数据是可选的.</p>
<p>下面是我们第一个Spider类代码，把它保存到<code>tutorial/spiders</code>目录下的<code>quotes_spider.py</code>文件中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        urls = [</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/2/'</span>,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        page = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>]</span><br><span class="line">        filename = <span class="string">'quotes-%s.html'</span> % page</span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">'Saved file %s'</span> % filename)</span><br></pre></td></tr></table></figure>
<p>我们可以看到，我们的Spider是<code>scrapy.Spider</code>的子类，我们在<code>QuotesSpider中</code>定义了一些属性和方法:</p>
<ul>
<li><p><a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.name" target="_blank" rel="noopener"><code>name</code></a>: identifies the Spider. It must be <strong>unique within a project</strong>, that is, you can’t set the same name for different Spiders.</p>
</li>
<li><p><a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.start_requests" target="_blank" rel="noopener"><code>start_requests()</code></a>: <strong>must return an iterable of Requests</strong> (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent(随后的) requests will be generated successively(依次) from these initial requests.</p>
</li>
<li><p><a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.parse" target="_blank" rel="noopener"><code>parse()</code></a>: a method that will be called to handle the response downloaded for each of the requests made. <strong>The response parameter</strong> is an instance of <a href="https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.TextResponse" target="_blank" rel="noopener"><code>TextResponse</code></a> that holds <strong>the page content</strong> and has further helpful methods to handle it.</p>
<p>The <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.parse" target="_blank" rel="noopener"><code>parse()</code></a> method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (<a href="https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request" target="_blank" rel="noopener"><code>Request</code></a>) from them.</p>
</li>
</ul>
<h2 id="运行我们的spider"><a href="#运行我们的spider" class="headerlink" title="运行我们的spider"></a>运行我们的spider</h2><p>进入项目顶级目录然后运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure>
<p>这条命令运行名字叫<code>quotes</code>的spider,这会发送一些请求到<code>quotes.toscrap.com</code>域名。你会得到类似下面这样的输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">2018-05-02 16:34:31 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot:</span><br><span class="line">2018-05-02 16:34:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, lib</span><br><span class="line">it (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptograph</span><br><span class="line">2018-05-02 16:34:31 [scrapy.crawler] INFO: Overridden settings: &#123;&apos;BOT_NA</span><br><span class="line">2018-05-02 16:34:31 [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">[&apos;scrapy.extensions.corestats.CoreStats&apos;,</span><br><span class="line"> &apos;scrapy.extensions.telnet.TelnetConsole&apos;,</span><br><span class="line"> &apos;scrapy.extensions.logstats.LogStats&apos;]</span><br><span class="line">2018-05-02 16:34:32 [scrapy.middleware] INFO: Enabled downloader middlew</span><br><span class="line">[&apos;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;]</span><br><span class="line">2018-05-02 16:34:32 [scrapy.middleware] INFO: Enabled spider middlewares</span><br><span class="line">[&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;]</span><br><span class="line">2018-05-02 16:34:32 [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[]</span><br><span class="line">2018-05-02 16:34:32 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2018-05-02 16:34:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (</span><br><span class="line">2018-05-02 16:34:32 [scrapy.extensions.telnet] DEBUG: Telnet console lis</span><br><span class="line">2018-05-02 16:34:35 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http:</span><br><span class="line">2018-05-02 16:34:35 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http:</span><br><span class="line">2018-05-02 16:34:35 [quotes] DEBUG: Saved file quotes-1.html</span><br><span class="line">2018-05-02 16:34:36 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http:</span><br><span class="line">2018-05-02 16:34:36 [quotes] DEBUG: Saved file quotes-2.html</span><br><span class="line">2018-05-02 16:34:36 [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line">2018-05-02 16:34:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;&apos;downloader/request_bytes&apos;: 678,</span><br><span class="line"> &apos;downloader/request_count&apos;: 3,</span><br><span class="line"> &apos;downloader/request_method_count/GET&apos;: 3,</span><br><span class="line"> &apos;downloader/response_bytes&apos;: 5976,</span><br><span class="line"> &apos;downloader/response_count&apos;: 3,</span><br><span class="line"> &apos;downloader/response_status_count/200&apos;: 2,</span><br><span class="line"> &apos;downloader/response_status_count/404&apos;: 1,</span><br><span class="line"> &apos;finish_reason&apos;: &apos;finished&apos;,</span><br><span class="line"> &apos;finish_time&apos;: datetime.datetime(2018, 5, 2, 8, 34, 36, 220916),</span><br><span class="line"> &apos;log_count/DEBUG&apos;: 6,</span><br><span class="line"> &apos;log_count/INFO&apos;: 7,</span><br><span class="line"> &apos;response_received_count&apos;: 3,</span><br><span class="line"> &apos;scheduler/dequeued&apos;: 2,</span><br><span class="line"> &apos;scheduler/dequeued/memory&apos;: 2,</span><br><span class="line"> &apos;scheduler/enqueued&apos;: 2,</span><br><span class="line"> &apos;scheduler/enqueued/memory&apos;: 2,</span><br><span class="line"> &apos;start_time&apos;: datetime.datetime(2018, 5, 2, 8, 34, 32, 396434)&#125;</span><br><span class="line">2018-05-02 16:34:36 [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure>
<p>现在检查一下当前文件夹中的文件，你应该注意到有两个新的文件被创建:<em>quotes-1.html</em> and <em>quotes-2.html</em>,其中保存着相应URLS的内容，这是解析函数(parse)的功能。</p>
<blockquote>
<p>如果你疑惑我们为啥还不解析HTML，请耐心一点，我们后边会讲</p>
</blockquote>
<h2 id="刚刚发生了什么？"><a href="#刚刚发生了什么？" class="headerlink" title="刚刚发生了什么？"></a>刚刚发生了什么？</h2><p>Scrapy调度Spider的<code>start_requests</code>方法返回的<code>scrapy.Request</code>对象。一旦收到响应，就实例化<code>Response</code>对象并调用与请求关联的回调方法,并将response作为参数传给回调函数</p>
<h2 id="start-requests方法的捷径"><a href="#start-requests方法的捷径" class="headerlink" title="start_requests方法的捷径"></a>start_requests方法的捷径</h2><p>你可以仅仅定义一个<code>start_urls</code>类属性来替代生成<code>start_urls</code>方法，这个属性会被默认生成的<code>start_urls</code>使用来创建你的爬虫的初始请求。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QuotesSpider(scrapy.Spider):</span><br><span class="line">	name=&quot;quotes&quot;</span><br><span class="line">	start_urls=[</span><br><span class="line">        &quot;http://quotes.toscrape.com/page/1&quot;,</span><br><span class="line">         &quot;http://quotes.toscrape.com/page/2&quot;,</span><br><span class="line">	]</span><br><span class="line">	</span><br><span class="line">	def parse(self,response):</span><br><span class="line">		page = response.url.split(&quot;/&quot;)[-2]</span><br><span class="line">		filename = &apos;quotes-%s.html&apos; % page</span><br><span class="line">		with open(filename,&apos;wb&apos;) as f:</span><br><span class="line">			f.write(response.body)</span><br></pre></td></tr></table></figure>
<p>The <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.parse" target="_blank" rel="noopener"><code>parse()</code></a> method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.parse" target="_blank" rel="noopener"><code>parse()</code></a> is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback. </p>
<h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><p>了解Scrapy如何提取数据的最好的方式是使用<strong>Scrapy Shell</strong>的selector,运行下面的命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell &apos;http://quotes.toscrape.com/page/1&apos;</span><br><span class="line">//在windows上用双引号替代单引号</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Remember to always enclose urls in quotes when running Scrapy shell from command-line, otherwise urls containing arguments (ie. <code>&amp;</code> character) will not work. </p>
</blockquote>
</p></div><div class="share"><span>分享到</span>&nbsp;<span class="soc"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></span><span class="soc"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></span><span class="soc"><a href="http://twitter.com/home?status=http://yoursite.com/2018/05/08/Scrapy食用指南2/%20Ginger's blog%20Scrapy食用指南2" class="fa fa-twitter"></a></span></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/2018/05/13/两只小爬虫/" title="两只小爬虫"><i class="fa fa-angle-double-left"></i>&nbsp;上一篇: 两只小爬虫</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/2018/05/08/Scrapy食用指南1/" title="Scrapy食用指南1">下一篇: Scrapy食用指南1&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2018&nbsp;<a target="_blank" href="http://yoursite.com" rel="noopener noreferrer">1-riverfish</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>