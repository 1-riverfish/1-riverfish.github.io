<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="1-riverfish"><title>Scrapy食用指南1 · Ginger's blog</title><meta name="description" content="Scrapy文档笔记–1
PRE  前言事情的经过是这样的:今年清明节左右，和两个同学参加了一个数据挖掘的比赛，我们的选题是影视推荐系统，嗯，温馨又从容。天杀的组委会给的数据，少的可怜的特征，于是我们把目光(屠刀)转向豆瓣，那么可怜，幼小又无助。在一番摸索(艰苦)之后，果断投入Scrapy框架的怀抱"><meta name="keywords" content=""><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/" class="a-title">Share&amp;Joy</a></h3><h1 tabindex="-1" class="site-title-large"><a href="/" class="a-title">Ginger' Blog</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li><li class="soc"><a href="https://github.com/1-riverfish" target="_blank" rel="noopener noreferrer"><i class="fa fa-github">&nbsp;</i></a><a href="http://yoursite.com/atom.xml" target="_blank" rel="noopener noreferrer"><i class="fa fa-rss">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2018&nbsp;<a target="_blank" href="http://yoursite.com" rel="noopener noreferrer">1-riverfish</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>Scrapy食用指南1</a></p><p class="post-meta"><span class="date meta-item">发布于&nbsp;2018-05-08</span><span class="meta-item"><i class="fa fa-tag"></i><span>&nbsp;</span><a href="/tags/Scrapy/" title="Scrapy" class="a-tag">Scrapy</a><span>&nbsp;</span></span></p><p class="post-abstract"><h1 id="Scrapy文档笔记–1"><a href="#Scrapy文档笔记–1" class="headerlink" title="Scrapy文档笔记–1"></a>Scrapy文档笔记–1</h1><p><img src="http://wx2.sinaimg.cn/mw690/0060lm7Tly1fqvwjc1apej309v03yt8k.jpg" alt=""></p>
<h2 id="PRE-前言"><a href="#PRE-前言" class="headerlink" title="PRE  前言"></a>PRE  前言</h2><p>事情的经过是这样的:今年清明节左右，和两个同学参加了一个数据挖掘的比赛，我们的选题是影视推荐系统，嗯，<strong>温馨又从容</strong>。天杀的组委会给的数据，少的可怜的特征，于是我们把目光(屠刀)转向豆瓣，那么<del>可怜，幼小又无助</del>。在一番摸索(艰苦)之后，果断投入Scrapy框架的怀抱。</p>
<h2 id="INTRO-介绍"><a href="#INTRO-介绍" class="headerlink" title="INTRO 介绍"></a>INTRO 介绍</h2><p>Scrapy是一个抓取网页提取有效数据的应用框架，它也可以被用来通过调用APIs来提取数据.</p>
<h2 id="EXAMPLE-例子"><a href="#EXAMPLE-例子" class="headerlink" title="EXAMPLE 例子"></a>EXAMPLE 例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://quotes.toscrape.com/tag/humor"</span>,</span><br><span class="line">        ]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'text'</span>: quote.css(<span class="string">'span.text::text'</span>).extract_first(),</span><br><span class="line">                <span class="string">'author'</span>: quote.xpath(<span class="string">'span/small/text()'</span>).extract_first(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        next_page = response.css(<span class="string">'li.next a::attr("href")'</span>).extract_first()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">yield</span> response.follow(next_page, self.parse)</span><br></pre></td></tr></table></figure>
<p>保存代码为<code>quotes_spider.py</code>，运行命令<code>runspider</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy runspider quotes_spider.py -o quotes.json</span><br></pre></td></tr></table></figure>
<p>程序终止后你会发现文件夹中多了<code>quotes.json</code>文件，就像下面这样</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[&#123;</span><br><span class="line">    <span class="attr">"author"</span>: <span class="string">"Jane Austen"</span>,</span><br><span class="line">    <span class="attr">"text"</span>: <span class="string">"\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d"</span></span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"author"</span>: <span class="string">"Groucho Marx"</span>,</span><br><span class="line">    <span class="attr">"text"</span>: <span class="string">"\u201cOutside of a dog, a book is man's best friend. Inside of a dog it's too dark to read.\u201d"</span></span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"author"</span>: <span class="string">"Steve Martin"</span>,</span><br><span class="line">    <span class="attr">"text"</span>: <span class="string">"\u201cA day without sunshine is like, you know, night.\u201d"</span></span><br><span class="line">&#125;,</span><br><span class="line">...]</span><br></pre></td></tr></table></figure>
<h3 id="What-just-happened-发生了什么"><a href="#What-just-happened-发生了什么" class="headerlink" title="What just happened? 发生了什么?"></a>What just happened? 发生了什么?</h3><p>当你运行 <code>scrapy runspider quotes_spider.py</code>时，Scrapy会在代码中寻找爬虫定义并且通过他的crawler engine运行代码。</p>
<p>爬取通过向<code>start_urls</code>属性中定义的URLs发出请求开始并且调用默认的callback方法<code>parse</code>,传递response对象作为一个参数.在回调函数<code>parse</code>中，我们使用CSS Selector遍历 quote 元素，用提取出来的 quote text和author来生成一个Python字典，寻找下一页的链接，安排另一个请求并使用相同的<code>parse</code>作为回调函数.</p>
<p>你会发现Scrapy的一个主要优点就是，请求的调度和处理是异步的，这意味着Scrapy不会等待一个请求完成然后处理它,在这段时间它可以发送另一个请求或者做别的事情.这也意味着其他请求可以继续进行即使一些请求失败或者在处理过程中发生了一些错误.尽管这允许你做非常快的抓取(以一个容错的方式同时发送多个并发的请求).同时，Scrapy给你提供了一些设置选项以控制爬取的礼貌(我的理解是，不要同时搞太多请求以防止对方服务器搞崩溃).你可以设置每次请求间的下载延时，设置每个域名或者每个IP的并发请求数量，甚至可以使用自动限制扩展来自动完成这些事情.</p>
<blockquote>
<p>这里使用了反馈输出来生成 JSON 文件，你可以轻松地改变输出文件的格式(XML 或者 CSV)或者存储后端(FTP或者Amazon  S3).当然也可以写一个 item pipeline 以在数据库中存储 items.</p>
</blockquote>
<h3 id="What-else-其他特性"><a href="#What-else-其他特性" class="headerlink" title="What else? 其他特性"></a>What else? 其他特性</h3><p>Scrapy提供了许多强有力的特征</p>
<ul>
<li>Built-in support for <a href="https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors" target="_blank" rel="noopener">selecting and extracting</a> data from HTML/XML sources using extended CSS selectors and XPath expressions, with helper methods to extract using regular expressions.</li>
<li>An <a href="https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell" target="_blank" rel="noopener">interactive shell console</a> (IPython aware) for trying out the CSS and XPath expressions to scrape data, very useful when writing or debugging your spiders.</li>
<li>Built-in support for <a href="https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports" target="_blank" rel="noopener">generating feed exports</a> in multiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP, S3, local filesystem)</li>
<li>Robust encoding support and auto-detection, for dealing with foreign, non-standard and broken encoding declarations.</li>
<li><a href="https://docs.scrapy.org/en/latest/index.html#extending-scrapy" target="_blank" rel="noopener">Strong extensibility support</a>, allowing you to plug in your own functionality using <a href="https://docs.scrapy.org/en/latest/topics/signals.html#topics-signals" target="_blank" rel="noopener">signals</a> and a well-defined API (middlewares, <a href="https://docs.scrapy.org/en/latest/topics/extensions.html#topics-extensions" target="_blank" rel="noopener">extensions</a>, and <a href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline" target="_blank" rel="noopener">pipelines</a>).</li>
<li>Wide range of built-in extensions and middlewares for handling:<ul>
<li>cookies and session handling</li>
<li>HTTP features like compression, authentication, caching</li>
<li>user-agent spoofing</li>
<li>robots.txt</li>
<li>crawl depth restriction</li>
<li>and more</li>
</ul>
</li>
<li>A <a href="https://docs.scrapy.org/en/latest/topics/telnetconsole.html#topics-telnetconsole" target="_blank" rel="noopener">Telnet console</a> for hooking into a Python console running inside your Scrapy process, to introspect and debug your crawler</li>
<li>Plus other goodies like reusable spiders to crawl sites from <a href="https://www.sitemaps.org/index.html" target="_blank" rel="noopener">Sitemaps</a> and XML/CSV feeds, a media pipeline for <a href="https://docs.scrapy.org/en/latest/topics/media-pipeline.html#topics-media-pipeline" target="_blank" rel="noopener">automatically downloading images</a> (or any other media) associated with the scraped items, a caching DNS resolver, and much more!</li>
</ul>
<blockquote>
<p>遇到一点小问题，在网上查了一下，是因为Scrapy版本过低</p>
<p>AttributeError: ‘HtmlResponse’ object has no attribute ‘follow’</p>
</blockquote>
<h2 id="INSTALL-GUIDE-安装指南"><a href="#INSTALL-GUIDE-安装指南" class="headerlink" title="INSTALL GUIDE 安装指南"></a>INSTALL GUIDE 安装指南</h2><p>Scrapy支持Python 2.7/3.4及以上版本</p>
<p>如果你使用的是Anaconda或者Miniconda,你可以通过下面的方式获取最新的 scrapy 包.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge scrapy</span><br></pre></td></tr></table></figure>
<p>或者你足够熟悉 Python 包的安装管理，你可以通过<code>pip</code>安装scrapy及它的依赖.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install --upgrade pip  //升级pip</span><br><span class="line">pip install ipykernel				//一个重要依赖</span><br><span class="line">pip install Scrapy</span><br></pre></td></tr></table></figure>
<p>请注意，为了安装Scrapy,有时候得根据你的操作系统为一些Scrapy的依赖解决一些编译问题，所以确保检查过<a href="https://docs.scrapy.org/en/latest/intro/install.html#intro-install-platform-notes" target="_blank" rel="noopener">Platform specific installation notes</a></p>
<p>我们强烈推荐你在专用的  <strong>虚拟环境</strong>(dedicated virtualenv)  中安装 Scrapy以避免和你系统中的包冲突.</p>
<h3 id="一些最好知道的事情"><a href="#一些最好知道的事情" class="headerlink" title="一些最好知道的事情"></a>一些最好知道的事情</h3><p>Scrapy是用纯Python书写并且依赖几个关键的Python包</p>
<ul>
<li><a href="http://lxml.de/" target="_blank" rel="noopener">lxml</a>, an efficient XML and HTML parser</li>
<li><a href="https://pypi.python.org/pypi/parsel" target="_blank" rel="noopener">parsel</a>, an HTML/XML data extraction library written on top of lxml,</li>
<li><a href="https://pypi.python.org/pypi/w3lib" target="_blank" rel="noopener">w3lib</a>, a multi-purpose helper for dealing with URLs and web page encodings</li>
<li><a href="https://twistedmatrix.com/" target="_blank" rel="noopener">twisted</a>, an asynchronous networking framework</li>
<li><a href="https://cryptography.io/" target="_blank" rel="noopener">cryptography</a> and <a href="https://pypi.python.org/pypi/pyOpenSSL" target="_blank" rel="noopener">pyOpenSSL</a>, to deal with various network-level security needs</li>
</ul>
<p>The minimal versions which Scrapy is tested against are:</p>
<ul>
<li>Twisted 14.0</li>
<li>lxml 3.4</li>
<li>pyOpenSSL 0.14</li>
</ul>
<p>Scrapy may work with older versions of these packages but it is not guaranteed it will continue working because it’s not being tested against them.</p>
<p>Some of these packages themselves depends on non-Python packages that might require additional installation steps depending on your platform. Please check <a href="https://docs.scrapy.org/en/latest/intro/install.html#intro-install-platform-notes" target="_blank" rel="noopener">platform-specific guides below</a>.</p>
<p>In case of any trouble related to these dependencies, please refer to their respective installation instructions:</p>
<ul>
<li><a href="http://lxml.de/installation.html" target="_blank" rel="noopener">lxml installation</a></li>
<li><a href="https://cryptography.io/en/latest/installation/" target="_blank" rel="noopener">cryptography installation</a></li>
</ul>
<h3 id="使用虚拟环境-推荐"><a href="#使用虚拟环境-推荐" class="headerlink" title="使用虚拟环境(推荐)"></a>使用虚拟环境(推荐)</h3><p>我们推荐在所有平台上在<strong>专门的虚拟环境</strong>中安装Scrapy</p>
<ul>
<li>安装 virtualenv</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenv</span><br></pre></td></tr></table></figure>
<ul>
<li><p>安装 scrapy</p>
<ul>
<li><p>Windows</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//在 virtualenv 中 </span><br><span class="line">//conda install -c conda-forge scrapy</span><br></pre></td></tr></table></figure>
</li>
<li><p>Ubuntu 14.04 or above</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//不要使用 ubuntu 提供的 python-scrapy 太旧了</span><br><span class="line"></span><br><span class="line">//在 virtualenv 中</span><br><span class="line">//安装依赖</span><br><span class="line">sudo apt-get install python3 python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev</span><br><span class="line"></span><br><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="Virtualenv用户指南"><a href="#Virtualenv用户指南" class="headerlink" title="Virtualenv用户指南"></a>Virtualenv用户指南</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenv</span><br><span class="line">//我遇到了time out error </span><br><span class="line">//修改设置的超时时间 或者更改下载源</span><br></pre></td></tr></table></figure>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="Step1-创建目录"><a href="#Step1-创建目录" class="headerlink" title="Step1 创建目录"></a>Step1 创建目录</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir pythonV</span><br></pre></td></tr></table></figure>
<h4 id="Step2-创建一个独立的Python运行环境-scrapyEnv"><a href="#Step2-创建一个独立的Python运行环境-scrapyEnv" class="headerlink" title="Step2 创建一个独立的Python运行环境 scrapyEnv"></a>Step2 创建一个独立的Python运行环境 scrapyEnv</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">virtual --no-site-packages venv</span><br><span class="line">//参数–no-site-packages 已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境</span><br></pre></td></tr></table></figure>
<h4 id="Step3-进入独立的scrapyEnv环境目录下"><a href="#Step3-进入独立的scrapyEnv环境目录下" class="headerlink" title="Step3 进入独立的scrapyEnv环境目录下"></a>Step3 进入独立的scrapyEnv环境目录下</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//Ubuntu</span><br><span class="line">source scrapyEnv/bin/activate</span><br><span class="line"></span><br><span class="line">//windows下</span><br><span class="line">//获得执行权限</span><br><span class="line">Set-ExecutionPolicy -Scope CurrentUser AllSigned</span><br><span class="line">Set-ExecutionPolicy -Scope CurrentUser RemoteSigned</span><br><span class="line">./scrapyEnv/Scripts/activate</span><br></pre></td></tr></table></figure>
<h4 id="Step4-安装各种第三方包"><a href="#Step4-安装各种第三方包" class="headerlink" title="Step4 安装各种第三方包"></a>Step4 安装各种第三方包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br><span class="line">//如果 windows下用的是 Anaconda或者Miniconda 则可以使用 conda 进行安装  这种方法是错误的</span><br><span class="line">conda install -c conda-forge scrapy</span><br><span class="line"></span><br><span class="line">//正确的方法 </span><br><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>
<p>在这里遇到一点问题，安装scrapy的时候，报错</p>
<p><code>error: [WinError 3] 系统找不到指定的路径。: &#39;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\PlatformSDK\\lib&#39;</code></p>
<p>在网上找了一下方法，比较合适的有两种</p>
<ol>
<li><p>安装VS开发工具，也就是VS2015/2017,显然这种方法可行性比较低</p>
</li>
<li><p>在这个<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud" target="_blank" rel="noopener">网站</a>下载你需要的包的.whl文件，放到对应文件夹底下，我这里是<code>G:\PYTHON\VirtualEnv\scrapyEnv\Lib\site-packages\Twisted-18.4.0-cp36-cp36m-win_amd64.whl</code></p>
<p>之后运行命令</p>
<p><code>pip install Twisted-18.4.0-cp36-cp36m-win_amd64.whl</code></p>
<p>显示安装成功，再运行命令</p>
<p><code>pip install scrapy</code></p>
<p>显示安装成功，OK!</p>
</li>
</ol>
<h4 id="Step5-退出当前环境"><a href="#Step5-退出当前环境" class="headerlink" title="Step5 退出当前环境"></a>Step5 退出当前环境</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deactivate</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Some paths within the virtualenv are slightly different on Windows: scripts and executables on Windows go in <code>ENV\Scripts\</code> instead of <code>ENV/bin/</code> and libraries go in <code>ENV\Lib\</code> rather than <code>ENV/lib/</code>.</p>
<p>To create a virtualenv under a path with spaces in it on Windows, you’ll need the <a href="http://sourceforge.net/projects/pywin32/" target="_blank" rel="noopener">win32api</a> library installed.</p>
</blockquote>
</p></div><div class="share"><span>分享到</span>&nbsp;<span class="soc"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></span><span class="soc"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></span><span class="soc"><a href="http://twitter.com/home?status=http://yoursite.com/2018/05/08/Scrapy食用指南1/%20Ginger's blog%20Scrapy食用指南1" class="fa fa-twitter"></a></span></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/2018/05/08/Scrapy食用指南2/" title="Scrapy食用指南2"><i class="fa fa-angle-double-left"></i>&nbsp;上一篇: Scrapy食用指南2</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/2018/05/07/回归/" title="回归">下一篇: 回归&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2018&nbsp;<a target="_blank" href="http://yoursite.com" rel="noopener noreferrer">1-riverfish</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>